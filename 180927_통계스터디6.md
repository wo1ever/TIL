# 180927

## 통계스터디 

#### Machine Learning

- Unsupervised Learning (비지도학습)
  - clustering: 데이터의 패턴을 학습시킴
  - 추천 알고리즘: 비슷한 사용자들을 묶어줌
- Supervised Learning (지도학습)
  - 회귀분석같은 경우, X들로 특정한 Y를 예측하기 위함 (입력값과 정답값을 함께 입력)
  - 예측값과 실제 값을 오차를 줄이는 방향으로 학습하여 정답을 맞추는 것
  - Classification: Y값의 카테고리 값들로 이루어진 범주형인 경우
  - Regression: Y값이 수치적인 연속형 값들로 이루어진 경우
- Reinforcement Learning (강화학습)
  - 정답을 맞추고/ 안맞추고의 문제 외에, 정답을 맞출 때마다 그 모델에 보상을 준다면, 그 보상이 큰 방향으로 학습을 하게 됨 (시스템상에서 가장 좋은 모델을 찾아냄)



#### KNN(K-Nearest Neighbors)

X값이 x0로 주어졌을 때의 Y=j 인 확률 = 전체 K개 중 y=j인 갯수를 셈.

유연성(k=1)과 안정성(k=100)의 trade-off (Overfitting VS Underfitting)

k가 작아질수록 training error는 작아지지만, test error는 줄어들다가 증가하는 경향.

knn특징:

- 단순하고 인과관계가 논리적으로 눈에 보임
- 로컬 정보에 최적화 되어 있음. 전체 데이터를 알 수 없어도 부분의 데이터만 있다면 어느정도 추측이 가능
- 차원의 저주에 걸리기 쉬움.
- 관측치가 적은 경우, 모수적 방법을 쓰지 않아도 진행할 수 있음.



#### Decision tree (의사결정나무)

- 분류나무: 예측값이 범주형인 경우
- 회귀나무: 예측값이 연속형인 경우

나누어진 영역들 안에서의 순도는 증가, 불순도/불확실성은 최대한 감소하는 방향으로 학습

[참고자료](https://ratsgo.github.io/machine%20learning/2017/03/26/tree/ )

Decision tree 특징:

- 결과를 해석하고 이해하기 쉬움. 간략한 설명만으로 결정트리를 이해하는 것이 가능
- 자료를 가공할 필요가 없음
- 수치 / 범주형 자료 모두에게 적용 가능
- 화이트박스 모델을 사용
  - 블랙박스 모델: 인과관계를 추정할 수 없다는 단점. 컴퓨터가 알아서 최적화시켜 과정은 볼 수 없어서 결과만 볼 수 있음. 대표적으로 머신러닝이 있음.
  - 화이트박스 모델: 중간 과정이 투명하게 공개됨. 왜 그 가지로 분류되었고, 등의 과정들을 통해 인과관계를 보기 쉬움.

Decision tree의 단점:

- 경계가 데이터 축에 수직이어서 비선형 데이터에는 적합하지 않음.
  - 이 문제를 극복하기 위해 랜덤포레스트 모델이 등장.
  - 의사결정 나무를 여러 모델로 만들어봐서 비교하면 됨.



#### [K-means Clustering](http://norman3.github.io/prml/docs/chapter09/1.html)

대표적인 비지도 학습 방법

k개의 평균을 가지고 군집을 만들어서 분류하는 방법

1. 분류가 잘 안 된 상태
   - 파란색/빨간색 X가 초기값 (랜덤한 지점)
   - 어느 X에 가까운지 거리를 재어 그룹을 임의로 나눔
2. 나눈 결과
3. 그 값들을 기준으로, 빨/파의 평균좌표가 각각 다시 조정됨.
4. 다시 그 지점들을 기준으로 파랑과 빨강에 가까운 지점들로 다시 나누어 짐
5. 나누어진 지점들의 좌표 평균이 재조정됨.
6. ...
7. ...
8. ...
9. 더 이상 값이 변하지 않는 지점으로 수렴

단점:

- 평균을 사용하기 때문에 아웃라이어에 취약함
  - 중앙값을 사용하여 계산 되기도 함



