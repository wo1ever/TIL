# 180928

## 추천시스템

### 추천시스템이란?

#### 추천시스템(Recommandation system)

특정 사용자가 관심을 가질만한 정보(영화, 음악, 책, 뉴스, 이미지, 웹페이지 등)를 추천하는 것

### 추천 시스템 종류

#### 추천 대상에 따른 추천시스템 분류

- 전체 추천
- 그룹 추천
- 개인 추천
- 아이템 기반 추천

### 전체 추천

개념: 불특정 다수에 대한 무작위 추천

예시: 네이버 메인 화면에 있는 글/이미지/상품, 광고배너, 실시간 검색어 등

특징: 이력이 없는 신규 이용자에게도 추천 가능

### 그룹 추천

개념: 사용자를 특정 그룹으로 나눈 후, 그에 특화된 추천을 제공

예시: 성/연령별/지역별 선호 상품, 키워드 등

### 개인 추천

개념: 사용자의 이력을 바탕으로 관심사를 추측해 그것에 맞는 아이템을 추측

예시: 일반적으로 Collaborate filtering(협업 필터링)이 많이 쓰임

특징: 사용자를 어떻게 모델링하냐에 따라 추천 성능이 달라짐

### 아이템 기반 추천

개념: 개별 아이템별로 관련 아이템을 보여주는 형태

예시: 최근 본 유사한 상품 탭, 유투브 연관 동영상 등



### 전체 추천

#### Popularity, High Rated Based

인기도, 높은 평점을 갖는 item을 추천

### 개인 추천

#### 협업 필터링(Collaborative filtering)

많은 사용자들로부터 얻은 기호정보에 따라 사용자들의 관심사들을 자동적으로 예측하게 해 주는 방법

- Neighborhood: 사용자나 상품 기준으로 유사성을 살핌
  - User-based
  - Item-based
- Latent Factor Models: 행렬의 수치적 특징을 이용하는 방법

### 협업 필터링

#### User-based CF

해당 사용자와 유사한 사용자를 찾은 후 이를 기반으로 item을 추천

1. user & item matrix 만들기

2. 해당 user와 기존 user들의 유사도 측정

3. 유사도를 기준으로 item 점수 계산하기

4. 높은 점수를 갖는 item 추천

#### Item-based CF

아이템간의 유사도를 구하고 이를 기반으로 item을 추천

1. item & user matrx 만들기
2. 해당 item과 다른 item들의 유사도 측정
3. 유사도를 기준으로 item 점수 계산하기
4. 높은 점수를 갖는 item 추천

#### 각각의 특징

User-based 추천

- 사용자에 대한 정보가 정확한 경우 추천성능이 올라감
- 사용자에 대한 정보가 정확하지 않은 경우 추천 성능이 엄청 떨어짐
- 계산 속도가 느림
- 데이터의 크기가 작고, 각 사용자에 대한 정보가 있는 경우 선호됨

Item-based 추천

- 계산 속도가 빠르지만 정확도가 떨어짐
- 추천 성능이 엄청 떨어지는 경우가 적음
- 데이터가 크거나 사용자에 대한 정확한 정보가 없는 경우 선호됨

### 잠재요인모델

#### Latent Factor Models

- 복잡한 사용자/영화 특성을 몇 개의 벡터로 간략화 하는 모델
- Matrix Factorization 방법이 주로 쓰이고, 이 방법은 user/item matrix를 user행렬과 item행렬의 곱으로 이루어진 행렬로 보는 것

### Matrix Factorization

\<User matrix> A: 1.2(코미디에 대한 A의 선호도), 0.8(액션에 대한 A의 선호도)

\<Item matrix> W: 1.5(W영화 안에 포함된 코미디의 요소), 1.7(W영화 안에 포함된 액션의 요소)

\<A의 W영화에 대한 선호 점수> (1.2 * 1.5) + (0.8 * 1.7) = 3.16

- 구하는 방법? 행렬의 분해 방법 중 하나인 'SVD'이용

### 행렬의 이해와 연산

#### 행렬(matrix)

#### Operator

- 행렬의 합 & 차: 각 원소들의 합 & 차를 계산하면 됨
- 행렬의 곱: 각 행렬의 행 벡터와 열 벡터의 곱(내적)
  - 주의: 앞의 행렬의 열의 개수와 뒤 행렬의 행의 개수가 같아야 함 (앞 행렬의 행의 원소의 수와 뒤 행렬의 열의 원소의 수가 같아야 함)

### 행렬의 종류

#### 영행렬(zero matrix)

mxn 행렬 A=aij이 있을 때, 모든 i, j에 대하여 aij=0(모든 원소가 0)인 행렬을 영행렬이라 한다.

#### 정방행렬(square matrix), 대각행렬(diagonal matrix)

mxn 행렬 A=aij이 있을 때, m=n인 행렬을 정방행렬이라고 한다. (n차 정방행렬)

n을 행렬의 차수(order)라고 함. 또 대각선 상의 원소들을 대각원소(diagonal element)라고 하며, 대각원소 이외의 모든 원소가 0인 행렬을 대각행렬(diagonal matrix)라고 한다.

#### 단위행렬

정방행렬이면서 대각원소가 모두 1이고, 그 외의 모든 원소가 0인 행렬을 단위행렬(I, unit/identity matrix)이라 함.

- 단위 행렬의 특징: 연산(행렬의 곱셈)의 결과가 자기 자신이 됨.

#### 역행렬

#### 전치행렬

#### 대칭행렬

#### 직교행렬

### 행렬의 분해

#### 특이값 분해(Singular Value Decomposition)

- m x n 크기의 행렬 R을 다음과 같은 세 행렬의 곱으로 나타내는 것
  - U = mxm인 직교행렬
  - sigma = 비대각 성분이 0인 mxn 행렬
  - V = nxn인 직교행렬
- sigma의 대각 성분을 특이치라고 하며 전체 특이치 중에거 가장 값이 큰 k개의 특이치만을 사용하여 (Truncated SVD) 다음가ㅗ 같은 행렬을 만듬

### PCA란?

#### 주성분분석 (Principal Component Analysis)

원래의 데이터 정보를 유지하면서 낮은 차원의 데이터(주성분)을 찾아내는 방법

### PCA 사례

#### 차원의 저주

차원의 증가에 따라 데이터가 sparse해지고 이로 인해 분류기의 성능이 떨어지는 현상

#### 차원축소(dimension reduction)

변수가 많을 시 차원의 저주의 위험이 존재. 따라서 차원을 줄여야 함.

- 공간의 차원이 증가함에 따라 sparsity가 증가함 → 데이터의 전반적인 구조를 유지하면서 차원을 축소

미스코리아 대회에 출전한 20명의 얼굴을 대상으로 얼마나 공통점이 많을까 PCA로 압축할 수 있음(만약 모두 개성이 넘친다면 20개의 독립적인 얼굴들이 표현될 것)

→ PCA결과 6개의 대표적인 얼굴(eigenface)이면 충분. 즉, 6개의 얼굴을 적절히 조합하면 20명의 얼굴을 모두 만들 수 있음을 의미

#### 다중공선성 제어

PCA를 수행하면 주성분들끼리는 수직이 됨. 즉, 회귀분석이나 tree모형에서 문제가 되는 설명변수 간의 다중공선성을 제거해 줌.

#### PC regression

PCA를 수행해서 나온 주성분을 설명번수로 사용하는 회귀모형

### PCA

#### PCA(Principal Comonent Analysis)란?

- 원래의 데이터 정보를 유지하면서 낮은 차원의 데이터(주성분)를 찾아내는 방법(고차원 → 저차원)
- 상관된 변수의 집합을 가능한 한 상관되지 않은 변수의 집합으로 변환하는 직교 선형 변환(상관관계가 있는 변수 → 상관관계가 없는 변수)
  - 첫 번째 주성분: 데이터의 변동(분산)을 가장 많이 설명하는 축을 선택
  - 두 번째 주성분: 첫 번째 주성분과 수직이면서 데이터의 변동(분산)을 가장 많이 설명하는 축을 선택
  - 세 번째 주성분: 첫 번째, 두 번째 주성분과 수직이면서 변동(분산)을 가장 많이 설명하는 축을 선택
  - ...

### PCA 유도

그렇다면, 주어진 데이터에 대해서 어떻게 하면 variability(분산)가 큰 성분을 찾을 수 있을 것인가?

#### 공분산 행렬

분산을 가지고 시행하기 때문에, 설명변수의 공분산 행렬을 구해보자.

- 공분산이란? 둘 이상의 자료에 대한 상관정도를 나타내는 값.(공분산을 표준화하면 '상관계수')

#### 공분산 행렬의 의미

1. 공분산 행렬은 각 자료(차원)가 얼마나 닮았는지를 표현
2. 각 자료들이 얼마나 함께 변하는지에 대해 말해줌

#### 스펙트럼분해 (고유값분해)

- V는 dxd인 직교행렬
- D는 dxd인 직교행렬

#### 고유값, 고유벡터

- 고유값(eigencalues)

  : 주성분의 중요도(데이터의 정보량), 값이 클수록 주성분의 중요도가 큼

- 고유벡터(eigenvectors)

  : 데이터가 어떤 방향으로 분산되어 있는지를 나타내 줌

#### 주성분(principal component)

주성분: XcV = Z = [z1,z2,...,zd]

- 새로운 설명변수의 집합이며, 주 성분들 각각의 상관계수는 0
- 주성분들의 분산은 1>2>…>d 번째를 만족

### 주성분 회귀모형

- 회귀분석, 설명변수 집합: X = [x1,x2,...,xd]

- 주성분 회귀모형

  : 설명변수 대신에 서로 독립인 주성분 Z = [z1,z2,...,zd]로 회귀모형 적합

- 설명변수의 개수만큼 주성분이 생기지만, 주성분을 설명변수의 개수보다 적게 사용한다 (m<p)

### 주성분 선택 방법

#### Cumulative Proportion

Cumulative Proportion의 값이 80~90% 이상인 고유값을 선택함

#### Scree plot

x축에는 dimensions, y축에는 해당 dimension의 eigenvalue를 그린 그래프로 그래프가 완만해 지기 시작하는 차원으로 선택

### PCA 회귀모형 정리

#### PCA 회귀모형이란?

- 주성분을 설명변수로 사용하는 회귀모형
- 설명변수의 변환을 위해 행렬의 분해(스펙트럼 분해)가 이용됨

#### 장점

1. 차원 축소(dimension reduction)
2. 다중공선성 perfect하게 해결

#### 단점

1. 주성분 자체에 대한 해석이 쉽지 않음
2. 주성분의 분산이 큰 순서대로 중요한 설명변수라는 보장이 없음